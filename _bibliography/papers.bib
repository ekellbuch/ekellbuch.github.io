---
---

@article{Harbor_Framework_Team_Harbor_A_framework_2026,
      author = {{Harbor Framework Team}},
      title = {{Harbor: A framework for evaluating and optimizing agents and models in container environments}},
      journal = {In preparation},
      code = {https://github.com/laude-institute/harbor},
      website = {https://harborframework.com/},
      year = {2026},
      abstract={Harbor is a framework for evaluating and optimizing agents and models in container environments.}
}


@inproceedings{tbench_2026,
      title={Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces}, 
      author={The Terminal-Bench Team},
      booktitle={ICLR},
      year={2026},
      html={https://arxiv.org/abs/2601.11868},
      code={https://github.com/laude-institute/terminal-bench},
      website={https://www.tbench.ai/},
      selected={true},
      abstract={AI agents may soon become capable of autonomously completing valuable, longhorizon tasks in diverse domains. Current benchmarks either do not measure real-world tasks, or are not sufficiently difficult to meaningfully measure frontier models. To this end, we present Terminal-Bench 2.0: a carefully curated hard benchmark composed of 89 tasks in computer terminal environments inspired by problems from real workflows. Each task features a unique environment, humanwritten solution, and comprehensive tests for verification. We show that frontier models and agents score less than 65% on the benchmark and conduct an error analysis to identify areas for model and agent improvement. We publish the dataset and evaluation harness to assist developers and researchers in future work at tbench.ai.},
} 


@article{gonzalez2026unifyingframeworkparallelizingsequential,
      title={A Unifying Framework for Parallelizing Sequential Models with Linear Dynamical Systems}, 
      author={Xavier Gonzalez* and E. Kelly Buchanan* and Hyun Dong Lee and Jerry Weihong Liu and Ke Alexander Wang and David M. Zoltowski and Christopher Ré and Scott W. Linderman},
      journal={TMLR},
      year={2026},
      html={https://arxiv.org/abs/2509.21716},
      code={https://github.com/lindermanlab/lds_fxd_pts},
      selected={true},
      abstract={Harnessing parallelism in seemingly sequential models is a central challenge for modern machine learning. Several approaches have been proposed for evaluating sequential processes in parallel using iterative fixed-point methods, like Newton, Picard, and Jacobi iterations. In this work, we show that these methods can be understood within a common framework based on linear dynamical systems (LDSs), where different iteration schemes arise naturally as approximate linearizations of a nonlinear recursion. Moreover, we theoretically analyze the rates of convergence of these methods, and we verify the predictions of this theory with several case studies. This unifying framework highlights shared principles behind these techniques and clarifies when particular fixed-point methods are most likely to be effective. By bridging diverse algorithms through the language of LDSs, the framework provides a clearer theoretical foundation for parallelizing sequential models and points toward new opportunities for efficient and scalable computation.}
}


@inproceedings{
saad-falcon2025weaver,
title={Weaver: Shrinking the Generation-Verification Gap by Scaling Compute for Verification},
author={Jon Saad-Falcon* and E. Kelly Buchanan* and Mayee F. Chen* and Tzu-Heng Huang and Brendan McLaughlin and Tanvir Bhathal and Shang Zhu and Ben Athiwaratkun and Frederic Sala and Scott Linderman and Azalia Mirhoseini and Christopher Ré},
booktitle={NeurIPS},
year={2025},
html={https://openreview.net/forum?id=dRjt4vlYVQ},
code={https://github.com/HazyResearch/scaling-verification},
blog={https://hazyresearch.stanford.edu/blog/2025-06-18-weaver},
selected={true},
abstract={Verifiers can improve language model (LM) capabilities by providing feedback or selecting the best response from a pool of generated candidates. Currently, high-quality verifiers are either unscalable (e.g., humans) or limited in utility (e.g., tools like Lean for formal proofs). While LM judges and reward models have become broadly useful as general-purpose verifiers, a significant performance gap remains between them and oracle verifiers. To help close this gap, we introduce Weaver, a framework for designing a strong verifier by combining multiple weak, imperfect verifiers. First we find that weighted ensembles of verifiers, which typically require learning from labeled data, significantly outperform unweighted combinations due to differences in the verifiers. To reduce the dependency on labeled data, Weaver leverages weak supervision to estimate each verifier’s accuracy and combines their outputs into a unified score that better reflects true response quality. However, directly applying weak supervision algorithms poses several challenges, including inconsistent verifier output formats and handling low-quality verifiers. Weaver addresses these challenges by using dataset statistics to normalize outputs and filter specific verifiers. We study the effectiveness of Weaver in repeated sampling settings, where a model generates multiple candidate responses at test time and a verifier is used to select the correct one. Our evaluations demonstrate that Weaver significantly improves the pass@1 performance across several reasoning and math tasks, achieving o3-mini level accuracy with Llama 3.3 70B Instruct (a much cheaper non-reasoning model) as the generator, and an ensemble of smaller judge and reward models as the verifiers (86.2% average). This gain mirrors the jump achieved between GPT-4o and o3-mini (69.0% vs. 86.7%), which required extensive finetuning and post-training interventions. To make Weaver more efficient, we train a compact 400M cross-encoder using Weaver's combined output scores. This distilled model retains 98.7% of Weaver's full accuracy while reducing verification compute by up to 99.97%.}
}


@article{nair2025batik,
      title={Batik: Behavior discovery, interpretation and annotation directly from raw video using video LLMs},
      author={Aditya Nair and Rohan Kolhe and Nestor Coria and Jadon Hale and Jineun Kim and Angel Wang and Amit Vinograd and Dan Biderman and E. Kelly Buchanan and Pietro Perona and Scott Linderman},
      journal={Under Review, Nature Methods},
      year={2025}
}



@inproceedings{
jiang2025extracting,
title={Extracting task-relevant preserved dynamics from contrastive aligned neural recordings},
author={Yiqi Jiang and Kaiwen Sheng and Yujia Gao and E. Kelly Buchanan and Yu Shikano and Seung Je Woo and Yixiu Zhao and Tony Hyun Kim and Fatih Dinc and Scott Linderman and Mark Schnitzer},
booktitle={NeurIPS},
year={2025},
html={https://openreview.net/forum?id=uvTea5Rfek},
code={https://github.com/schnitzer-lab/CANDY-public},
note={Spotlight Presentation},
abstract={Recent work indicates that low-dimensional dynamics of neural and behavioral data are often preserved across days and subjects. However, extracting these preserved dynamics remains challenging: high-dimensional neural population activity and the recorded neuron populations vary across recording sessions. While existing modeling tools can improve alignment between neural and behavioral data, they often operate on a per-subject basis or discretize behavior into categories, disrupting its natural continuity and failing to capture the underlying dynamics. We introduce Contrastive Aligned Neural DYnamics (CANDY), an end-to-end framework that aligns neural and behavioral data using rank-based contrastive learning, adapted for continuous behavioral variables, to project neural activity from different sessions onto a shared low-dimensional embedding space. CANDY fits a shared linear dynamical system to the aligned embeddings, enabling an interpretable model of the conserved temporal structure in the latent space. We validate CANDY on synthetic and real-world datasets spanning multiple species, behaviors, and recording modalities. Our results show that CANDY is able to learn aligned latent embeddings and preserved dynamics across neural recording sessions and subjects, and it achieves improved cross-session behavior decoding performance. We further show that the latent linear dynamical system generalizes to new sessions and subjects, achieving comparable or even superior behavior decoding performance to models trained from scratch. These advances enable robust cross-session behavioral decoding and offer a path towards identifying shared neural dynamics that underlie behavior across individuals and recording conditions.}
}


@article{ibl2025brainwide,
title={Brain-wide representations of prior information in mouse decision-making},
author={Charles Findling and Félix Hubert and International Brain Laboratory},
journal={Nature},
volume={645},
number={8079},
pages={192--200},
year={2025},
doi={10.1038/s41586-025-09226-1},
html={https://www.nature.com/articles/s41586-025-09226-1},
code={https://github.com/int-brain-lab/prior-localization},
selected={false},
abstract={The neural representations of prior information about the state of the world are poorly understood. Here, to investigate them, we examined brain-wide Neuropixels recordings and widefield calcium imaging collected by the International Brain Laboratory. Mice were trained to indicate the location of a visual grating stimulus, which appeared on the left or right with a prior probability alternating between 0.2 and 0.8 in blocks of variable length. We found that mice estimate this prior probability and thereby improve their decision accuracy. Furthermore, we report that this subjective prior is encoded in at least 20% to 30% of brain regions that, notably, span all levels of processing, from early sensory areas (the lateral geniculate nucleus and primary visual cortex) to motor regions (secondary and primary motor cortex and gigantocellular reticular nucleus) and high-level cortical regions (the dorsal anterior cingulate area and ventrolateral orbitofrontal cortex). This widespread representation of the prior is consistent with a neural model of Bayesian inference involving loops between areas, as opposed to a model in which the prior is incorporated only in decision-making areas. This study offers a brain-wide perspective on prior encoding at cellular resolution, underscoring the importance of using large-scale recordings on a single standardized task.}
}


@article{ibl2025reproducibility,
title={Reproducibility of in vivo electrophysiological measurements in mice},
author={International Brain Laboratory},
journal={eLife},
volume={13},
pages={RP100840},
year={2025},
doi={10.7554/eLife.100840},
html={https://elifesciences.org/articles/100840},
code={https://github.com/int-brain-lab/paper-reproducible-ephys/},
selected={false},
abstract={Understanding brain function relies on the collective work of many labs generating reproducible results. However, reproducibility has not been systematically assessed within the context of electrophysiological recordings during cognitive behaviors. To address this, we formed a multi-lab collaboration using a shared, open-source behavioral task and experimental apparatus. Experimenters in ten laboratories repeatedly targeted Neuropixels probes to the same location (spanning secondary visual areas, hippocampus, and thalamus) in mice making decisions; this generated a total of 121 experimental replicates, a unique dataset for evaluating reproducibility of electrophysiology experiments. Despite standardizing both behavioral and electrophysiological procedures, some experimental outcomes were highly variable. A closer analysis uncovered that variability in electrode targeting hindered reproducibility, as did the limited statistical power of some routinely used electrophysiological analyses, such as single-neuron tests of modulation by task parameters. Reproducibility was enhanced by histological and electrophysiological quality-control criteria. Our observations suggest that data from systems neuroscience is vulnerable to a lack of reproducibility, but that across-lab standardization, including metrics we propose, can serve to mitigate this.}
}


@inproceedings{
saad-falcon2025an,
title={An Architecture Search Framework for Inference-Time Techniques},
author={Jon Saad-Falcon and Adrian Gamarra Lafuente and Shlok Natarajan and Nahum Maru and Hristo Todorov and Etash Kumar Guha and E. Kelly Buchanan and Mayee F Chen and Neel Guha and Christopher Ré and Azalia Mirhoseini},
booktitle={ICML},
year={2025},
html={https://openreview.net/forum?id=EGrSMMj37o},
code={https://github.com/ScalingIntelligence/Archon},
selected={false},
abstract={Inference-time techniques, such as repeated sampling or iterative revisions, are emerging as powerful ways to enhance large-language models (LLMs) at test time. However, best practices for developing systems that combine these techniques remain underdeveloped due to our limited understanding of the utility of each technique across models and tasks, the interactions between them, and the massive search space for combining them. To address these challenges, we introduce Archon, a modular and automated framework for optimizing the process of selecting and combining inference-time techniques and LLMs. Given a compute budget and a set of available LLMs, Archon explores a large design space to discover optimized configurations tailored to target benchmarks. It can design custom or general-purpose architectures that advance the Pareto frontier of accuracy vs. maximum token budget compared to top-performing baselines. Across instruction-following, reasoning, and coding tasks, we show that Archon can leverage additional inference compute budget to design systems that outperform frontier models such as OpenAI's o1, GPT-4o, and Claude 3.5 Sonnet by an average of 15.1%.}
}


@article{
abe2024pathologies,
title={Pathologies of Predictive Diversity in Deep Ensembles},
author={Taiga Abe and E. Kelly Buchanan and Geoff Pleiss and John Patrick Cunningham},
journal={TMLR},
issn={2835-8856},
year={2024},
html={https://openreview.net/forum?id=TQfQUksaC8},
code={https://github.com/cellistigs/ensemble_attention/tree/dkl},
note={Featured Certification},
selected={false},
abstract={Classic results establish that encouraging predictive diversity improves performance in ensembles of low-capacity models, e.g. through bagging or boosting. Here we demonstrate that these intuitions do not apply to high-capacity neural network ensembles (deep ensembles), and in fact the opposite is often true. In a large scale study of nearly 600 neural network classification ensembles, we examine a variety of interventions that trade off component model performance for predictive diversity. While such interventions can improve the performance of small neural network ensembles (in line with standard intuitions), they harm the performance of the large neural network ensembles most often used in practice. Surprisingly, we also find that discouraging predictive diversity is often benign in large-network ensembles, fully inverting standard intuitions. Even when diversity-promoting interventions do not sacrifice component model performance (e.g. using heterogeneous architectures and training paradigms), we observe an opportunity cost associated with pursuing increased predictive diversity. Examining over 1000 ensembles, we observe that the performance benefits of diverse architectures/training procedures are easily dwarfed by the benefits of simply using higher-capacity models, despite the fact that such higher capacity models often yield significantly less predictive diversity. Overall, our findings demonstrate that standard intuitions around predictive diversity, originally developed for low-capacity ensembles, do not directly apply to modern high-capacity deep ensembles. This work clarifies fundamental challenges to the goal of improving deep ensembles by making them more diverse, while suggesting an alternative path: simply forming ensembles from ever more powerful (and less diverse) component models.}
}


@inproceedings{
abe2022deep,
title={Deep Ensembles Work, But Are They Necessary?},
author={Taiga Abe* and E. Kelly Buchanan*  and Geoff Pleiss and Richard Zemel and John Patrick Cunningham},
booktitle={NeurIPS},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
html={https://openreview.net/forum?id=Wl1ZIgMqLlq},
code={https://github.com/cellistigs/interp_ensembles},
selected={false},
abstract={Ensembling neural networks is an effective way to increase accuracy, and can often match the performance of individual larger models. This observation poses a natural question: given the choice between a deep ensemble and a single neural network with similar accuracy, is one preferable over the other? Recent work suggests that deep ensembles may offer distinct benefits beyond predictive power: namely, uncertainty quantification and robustness to dataset shift. In this work, we demonstrate limitations to these purported benefits, and show that a single (but larger) neural network can replicate these qualities. First, we show that ensemble diversity, by any metric, does not meaningfully contribute to an ensemble's uncertainty quantification on out-of-distribution (OOD) data, but is instead highly correlated with the relative improvement of a single larger model. Second, we show that the OOD performance afforded by ensembles is strongly determined by their in-distribution (InD) performance, and -- in this sense -- is not indicative of any "effective robustness". While deep ensembles are a practical way to achieve improvements to predictive power, uncertainty quantification, and robustness, our results show that these improvements can be replicated by a (larger) single model.}
}


@article{willett2024braintotext,
      title={Brain-to-Text Benchmark '24: Lessons Learned},
      author={Francis R. Willett and Jingyuan Li and Trung Le and Chaofei Fan and Mingfei Chen and Eli Shlizerman and Yue Chen and Xin Zheng and Tatsuo S. Okubo and Tyler Benster and Hyun Dong Lee and Maxwell Kounga and E. Kelly Buchanan and David Zoltowski and Scott W. Linderman and Jaimie M. Henderson},
      journal={arXiv},
      year={2024},
      html={https://arxiv.org/abs/2412.17227},
      code={https://github.com/ekellbuch/deep_ssm},
      abstract={Speech brain-computer interfaces aim to decipher what a person is trying to say from neural activity alone, restoring communication to people with paralysis who have lost the ability to speak intelligibly. The Brain-to-Text Benchmark '24 and associated competition was created to foster the advancement of decoding algorithms that convert neural activity to text. Here, we summarize the lessons learned from the competition ending on June 1, 2024 (the top 4 entrants also presented their experiences in a recorded webinar). The largest improvements in accuracy were achieved using an ensembling approach, where the output of multiple independent decoders was merged using a fine-tuned large language model (an approach used by all 3 top entrants). Performance gains were also found by improving how the baseline recurrent neural network (RNN) model was trained, including by optimizing learning rate scheduling and by using a diphone training objective. Improving upon the model architecture itself proved more difficult, however, with attempts to use deep state space models or transformers not yet appearing to offer a benefit over the RNN baseline. The benchmark will remain open indefinitely to support further work towards increasing the accuracy of brain-to-text algorithms.}
}

@inproceedings{buchanan2023effects,
title={The Effects of Ensembling on Long-Tailed Data},
author={E. Kelly Buchanan and Geoff Pleiss and Yixin Wang and John P. Cunningham},
booktitle={Heavy Tails in ML Workshop, NeurIPS},
year={2023},
html={https://openreview.net/forum?id=l4GYs60kre},
code={https://github.com/ekellbuch/longtail_ensembles},
selected={false},
abstract={Deep ensembles are a popular approach to improve accuracy and calibration over single model performance, either by averaging logits, or probabilities of multiple models. Recent theoretical work has shown that logit and probability ensembles have different effects on the model bias and variance, but to our knowledge these benefits have not yet been used to inform how to create ensembles. In this work, we show that for balanced datasets, there is no significant difference between logit and probability ensembles in terms of accuracy and ranked calibration. In contrast, we show that in long-tailed datasets, there are gains from logit ensembling when combined with imbalance bias reduction losses. In turn, our results show that we can have consistent performance improvements using loss-aware ensembles when dealing with long-tail data.}
}

@inproceedings{abe2022best,
title={The Best Deep Ensembles Sacrifice Predictive Diversity},
author={Taiga Abe* and E. Kelly Buchanan* and Geoff Pleiss and John P. Cunningham},
booktitle={NeurIPS I Can't Believe It's Not Better Workshop},
year={2022},
html={https://openreview.net/forum?id=6sBiAIpkUiO},
note={Entropic Award for Most Surprising Negative Result},
abstract={Ensembling remains a hugely popular method for increasing the performance of
a given class of models. In the case of deep learning, the benefits of ensembling
are often attributed to the diverse predictions of the individual ensemble members.
Here we investigate a tradeoff between diversity and individual model performance,
and find that—surprisingly—encouraging diversity during training almost always
yields worse ensembles. We show that this tradeoff arises from the Jensen gap
between the single model and ensemble losses, and show that Jensen gap is a
natural measure of diversity for both the mean squared error and cross entropy loss
functions. Our results suggest that to reduce the ensemble error, we should move
away from efforts to increase predictive diversity, and instead we should construct
ensembles from less diverse (but more accurate) component models.},
selected={false}
}


@article{abe2022neurocaas,
title={Neuroscience Cloud Analysis As a Service: An open-source platform for scalable, reproducible data analysis},
author={Taiga Abe and Ian Kinsella and Shreya Saxena and E. Kelly Buchanan and João Couto and John Briggs and Sian Lee Kitt and Ryan Glassman and John Zhou and Liam Paninski and John P. Cunningham},
journal={Neuron},
volume={110},
number={17},
pages={2771--2789.e7},
year={2022},
doi={10.1016/j.neuron.2022.06.018},
html={https://www.cell.com/neuron/fulltext/S0896-6273(22)00587-6},
code={https://github.com/cunningham-lab/neurocaas},
website={https://www.neurocaas.org/},
selected={false},
abstract={A key aspect of neuroscience research is the development of powerful, general-purpose data analyses that process large datasets. Unfortunately, modern data analyses have a hidden dependence upon complex computing infrastructure (e.g., software and hardware), which acts as an unaddressed deterrent to analysis users. Although existing analyses are increasingly shared as open-source software, the infrastructure and knowledge needed to deploy these analyses efficiently still pose significant barriers to use. In this work, we develop Neuroscience Cloud Analysis As a Service (NeuroCAAS): a fully automated open-source analysis platform offering automatic infrastructure reproducibility for any data analysis. We show how NeuroCAAS supports the design of simpler, more powerful data analyses and that many popular data analysis tools offered through NeuroCAAS outperform counterparts on typical infrastructure. Pairing rigorous infrastructure management with cloud resources, NeuroCAAS dramatically accelerates the dissemination and use of new data analyses for neuroscientific discovery.}
}


@inproceedings{tran2022plexreliabilityusingpretrained,
      title={Plex: Towards Reliability using Pretrained Large Model Extensions}, 
      author={Dustin Tran and Jeremiah Liu and Michael W. Dusenberry and Du Phan and Mark Collier and Jie Ren and Kehang Han and Zi Wang and Zelda Mariet and Huiyi Hu and Neil Band and Tim G. J. Rudner and Karan Singhal and Zachary Nado and Joost van Amersfoort and Andreas Kirsch and Rodolphe Jenatton and Nithum Thain and Honglin Yuan and E. Kelly Buchanan and Kevin Murphy and D. Sculley and Yarin Gal and Zoubin Ghahramani and Jasper Snoek and Balaji Lakshminarayanan},
      booktitle={ICML Pre-training Workshop},
      year={2022},
      html={https://arxiv.org/abs/2207.07411}, 
      code={https://github.com/google/uncertainty-baselines},
      blog={https://ai.googleblog.com/2022/11/plex-towards-reliability-using.html},
      note={Contributed Talk, 5.9% of accepted papers},
      selected={false},
      abstract={A recent trend in artificial intelligence is the use of pretrained models for language and vision tasks, which have achieved extraordinary performance but also puzzling failures. Probing these models' abilities in diverse ways is therefore critical to the field. In this paper, we explore the reliability of models, where we define a reliable model as one that not only achieves strong predictive performance but also performs well consistently over many decision-making tasks involving uncertainty (e.g., selective prediction, open set recognition), robust generalization (e.g., accuracy and proper scoring rules such as log-likelihood on in- and out-of-distribution datasets), and adaptation (e.g., active learning, few-shot uncertainty). We devise 10 types of tasks over 40 datasets in order to evaluate different aspects of reliability on both vision and language domains. To improve reliability, we developed ViT-Plex and T5-Plex, pretrained large model extensions for vision and language modalities, respectively. Plex greatly improves the state-of-the-art across reliability tasks, and simplifies the traditional protocol as it improves the out-of-the-box performance and does not require designing scores or tuning the model for each task. We demonstrate scaling effects over model sizes up to 1B parameters and pretraining dataset sizes up to 4B examples. We also demonstrate Plex's capabilities on challenging tasks including zero-shot open set recognition, active learning, and uncertainty in conversational language understanding.}
}

@inproceedings{NEURIPS2020_4379cf00,
 author = {Anqi Wu* and E. Kelly Buchanan* and Matthew Whiteway and Michael Schartner and Guido Meijer and Jean-Paul Noel and Erica Rodriguez and Claire Everett and Amy Norovich and Evan Schaffer and Neeli Mishra and C. Daniel Salzman and Dora Angelaki and Andr\'{e}s Bendesky and The International Brain Laboratory and Cunningham, John P and Paninski, Liam},
 booktitle = {NeurIPS},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {6040--6052},
 publisher = {Curran Associates, Inc.},
 title = {Deep Graph Pose: a semi-supervised deep graphical model for improved animal pose tracking},
 html = {https://openreview.net/forum?id=mVbTvfhOzEC},
 code = {https://github.com/paninski-lab/deepgraphpose},
 volume = {33},
 year = {2020},
 selected={false},
 abstract={Noninvasive behavioral tracking of animals is crucial for many scientific investigations. Recent transfer learning approaches for behavioral tracking have considerably advanced the state of the art. Typically these methods treat each video
frame and each object to be tracked independently. In this work, we improve on
these methods (particularly in the regime of few training labels) by leveraging the
rich spatiotemporal structures pervasive in behavioral video — specifically, the
spatial statistics imposed by physical constraints (e.g., paw to elbow distance),
and the temporal statistics imposed by smoothness from frame to frame. We propose a probabilistic graphical model built on top of deep neural networks, Deep
Graph Pose (DGP), to leverage these useful spatial and temporal constraints, and
develop an efficient structured variational approach to perform inference in this
model. The resulting semi-supervised model exploits both labeled and unlabeled
frames to achieve significantly more accurate and robust tracking while requiring
users to label fewer training frames. In turn, these tracking improvements enhance
performance on downstream applications, including robust unsupervised segmentation of behavioral “syllables,” and estimation of interpretable “disentangled”
low-dimensional representations of the full behavioral video.}
}


@article{buchanan2018penalized,
  title={Penalized matrix decomposition for denoising, compression, and improved demixing of functional imaging data},
  author={E. Kelly Buchanan* and Ian Kinsella* and Ding Zhou* and Rong Zhu and Pengcheng Zhou and Felipe Gerhard and John Ferrante and Ying Ma and Sharon H. Kim and Mohammed A. Shaik and others},
  journal={BioRxiv},
  pages={334706},
  year={2018},
  publisher={Cold Spring Harbor Laboratory},
  html={https://arxiv.org/abs/1807.06203},
  code={https://github.com/paninski-lab/funimag},
  selected={false},
  abstract={Calcium imaging has revolutionized systems neuroscience, providing the ability to image large neural populations with single-cell resolution. The resulting datasets are quite large, which has presented a barrier to routine open sharing of this data, slowing progress in reproducible research. State of the art methods for analyzing this data are based on non-negative matrix factorization (NMF); these approaches solve a non-convex optimization problem, and are effective when good initializations are available, but can break down in low-SNR settings where common initialization approaches fail. Here we introduce an approach to compressing and denoising functional imaging data. The method is based on a spatially-localized penalized matrix decomposition (PMD) of the data to separate (low-dimensional) signal from (temporally-uncorrelated) noise. This approach can be applied in parallel on local spatial patches and is therefore highly scalable, does not impose non-negativity constraints or require stringent identifiability assumptions (leading to significantly more robust results compared to NMF), and estimates all parameters directly from the data, so no hand-tuning is required. We have applied the method to a wide range of functional imaging data (including one-photon, two-photon, three-photon, widefield, somatic, axonal, dendritic, calcium, and voltage imaging datasets): in all cases, we observe ~2-4x increases in SNR and compression rates of 20-300x with minimal visible loss of signal, with no adjustment of hyperparameters; this in turn facilitates the process of demixing the observed activity into contributions from individual neurons. We focus on two challenging applications: dendritic calcium imaging data and voltage imaging data in the context of optogenetic stimulation. In both cases, we show that our new approach leads to faster and much more robust extraction of activity from the data.}
}